{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r'C:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search'\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract, Chunk and Index \n",
    "\n",
    "We are going to use two custom classses to help us remove the burden of code. We'll have two cleint one for extarcticn fsta and the ireh for chcinckon and indexidn on azure search\n",
    "\n",
    "### SharePointDataExtractor\n",
    "\n",
    "SharePointDataExtractor is a client designed to interact with Microsoft SharePoint through the Microsoft Graph API. It handles various tasks related to fetching and processing data from SharePoint sites.\n",
    "\n",
    "Key Functionalities:\n",
    "\n",
    "+ Authentication: Handles OAuth authentication with Microsoft Graph API using tenant ID, client ID, and client secret.\n",
    "- Data Retrieval: Fetches data from specific SharePoint sites and drives. This includes retrieving site IDs, drive IDs, and files within a SharePoint site.\n",
    "+ File Processing: Ability to filter files based on modification time and file formats. It can retrieve file contents, particularly from .docx files.\n",
    "- Permissions Handling: Fetches and processes file permissions to understand access control and roles associated with SharePoint files.\n",
    "- Data Extraction: Compiles detailed information about each file, including content, location, and user roles, into a structured format.\n",
    "\n",
    "Usage Context:\n",
    "This client is used when there is a need to extract and process data from SharePoint. It's particularly useful for applications that require automated retrieval and processing of documents and files stored in SharePoint.\n",
    "\n",
    "## TextChunkingIndexing\n",
    "\n",
    "TextChunkingIndexing is a client focused on processing and indexing text data. It primarily deals with chunking large text into manageable pieces and preparing it for indexing or further analysis.\n",
    "\n",
    "Key Functionalities:\n",
    "\n",
    "+ Environment Setup: Loads necessary environment variables required for indexing and chunking operations.\n",
    "- Text Chunking: Capable of breaking down large text data into smaller chunks based on character count, which is essential for text analysis and indexing in databases.\n",
    "+ Customization: Offers customization options for chunk size and overlap, making it versatile for various text processing needs.\n",
    "\n",
    "Usage Context:\n",
    "This client is particularly useful in scenarios where large text documents need to be processed, analyzed, or indexed. For example, in Natural Language Processing tasks, machine learning model training, or when preparing data for storage in databases where smaller text chunks are preferable.\n",
    "\n",
    "### Example Workflow:\n",
    "\n",
    "Use client_scrapping (SharePointDataExtractor) to retrieve documents from a SharePoint site.\n",
    "Pass these documents to client_indexing (TextChunkingIndexing) to break the text into smaller, more manageable chunks.\n",
    "Use the chunked text and indexing to our selected Vector Database Azure Search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbb_ai.sharepoint_data_extractor import SharePointDataExtractor\n",
    "from gbb_ai.langchain_indexing import TextChunkingIndexing\n",
    "\n",
    "# Instantiate the SharePointDataExtractor client\n",
    "# This client is responsible for connecting to Microsoft SharePoint through the Microsoft Graph API.\n",
    "# The client handles the complexities of interacting with SharePoint's REST API, providing an easy-to-use interface for data extraction.\n",
    "client_scrapping = SharePointDataExtractor()\n",
    "\n",
    "# Instantiate the TextChunkingIndexing client\n",
    "# This cleint is resposnsinle for chunking text into smaller pieces using Langchaing framework, which are then indexed by Azure Cognitive Search.\n",
    "# The client offers customizable options for how text should be chunked, ensuring flexibility to suit various text processing needs.\n",
    "client_indexing = TextChunkingIndexing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE_DOMAIN = 'mngenvmcap747548.sharepoint.com'\n",
    "SITE_NAME = 'Contoso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 20:16:39,991 - micro - MainProcess - INFO     New access token retrieved.... (sharepoint_data_extractor.py:msgraph_auth:58)\n",
      "2023-12-09 20:16:39,992 - micro - MainProcess - INFO     Decoded Access Token:\n",
      "{\n",
      "  \"aud\": \"https://graph.microsoft.com\",\n",
      "  \"iss\": \"https://sts.windows.net/9495d8c9-4ebb-4107-b905-c7b45d1b7b7a/\",\n",
      "  \"iat\": 1702174299,\n",
      "  \"nbf\": 1702174299,\n",
      "  \"exp\": 1702178199,\n",
      "  \"aio\": \"E2VgYHjqzM3iyTY70/+7wJ/X23clAQA=\",\n",
      "  \"app_displayname\": \"dev-graph\",\n",
      "  \"appid\": \"118583ee-94ed-45dd-870b-73784045eb37\",\n",
      "  \"appidacr\": \"1\",\n",
      "  \"idp\": \"https://sts.windows.net/9495d8c9-4ebb-4107-b905-c7b45d1b7b7a/\",\n",
      "  \"idtyp\": \"app\",\n",
      "  \"oid\": \"4f614374-65fa-45fc-8369-cb616a6fe08f\",\n",
      "  \"rh\": \"0.Ab0AydiVlLtOB0G5Bce0XRt7egMAAAAAAAAAwAAAAAAAAADLAAA.\",\n",
      "  \"roles\": [\n",
      "    \"TeamsActivity.Read.All\",\n",
      "    \"SharePointTenantSettings.Read.All\",\n",
      "    \"People.Read.All\",\n",
      "    \"Sites.Read.All\",\n",
      "    \"Sites.Manage.All\",\n",
      "    \"Directory.Read.All\",\n",
      "    \"OnlineMeetingTranscript.Read.All\",\n",
      "    \"BrowserSiteLists.ReadWrite.All\",\n",
      "    \"Files.Read.All\",\n",
      "    \"Mail.Read\",\n",
      "    \"Chat.Read.All\"\n",
      "  ],\n",
      "  \"sub\": \"4f614374-65fa-45fc-8369-cb616a6fe08f\",\n",
      "  \"tenant_region_scope\": \"NA\",\n",
      "  \"tid\": \"9495d8c9-4ebb-4107-b905-c7b45d1b7b7a\",\n",
      "  \"uti\": \"UEmV4xdje0qinXNl4eExAA\",\n",
      "  \"ver\": \"1.0\",\n",
      "  \"wids\": [\n",
      "    \"0997a1d0-0d1d-4acb-b408-d5ca73121e90\"\n",
      "  ],\n",
      "  \"xms_tcdt\": 1697638621\n",
      "} (sharepoint_data_extractor.py:msgraph_auth:74)\n",
      "2023-12-09 20:16:39,992 - micro - MainProcess - INFO     Token Expires at: 2023-12-09 21:16:39 (sharepoint_data_extractor.py:msgraph_auth:78)\n",
      "2023-12-09 20:16:39,994 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:120)\n",
      "2023-12-09 20:16:40,763 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:124)\n",
      "2023-12-09 20:16:41,477 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:169)\n",
      "2023-12-09 20:16:42,226 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:172)\n",
      "2023-12-09 20:16:42,226 - micro - MainProcess - INFO     Fetching content for file: test.docx (sharepoint_data_extractor.py:retrieve_sharepoint_files_content:334)\n",
      "2023-12-09 20:16:42,231 - micro - MainProcess - INFO     Starting request for file: test.docx (sharepoint_data_extractor.py:get_docx_content:271)\n",
      "2023-12-09 20:16:43,464 - micro - MainProcess - INFO     Successfully retrieved content for file: test.docx (sharepoint_data_extractor.py:get_docx_content:287)\n",
      "2023-12-09 20:16:44,244 - micro - MainProcess - INFO     Returning highest priority group: Group_critical (azure_search_security_trimming.py:get_highest_priority_group:47)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve .docx file contents from a specified SharePoint site using SharePointDataExtractor\n",
    "content_files = client_scrapping.retrieve_sharepoint_files_content(site_domain=SITE_DOMAIN, site_name=SITE_NAME, minutes_ago=None,file_formats=[\"docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_indexing.setup_aoai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 20:16:44,273 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model text-embedding-ada-002, deployment foundational-ada, and chunk size 1000 (langchain_indexing.py:load_embedding_model:103)\n",
      "2023-12-09 20:16:44,278 - micro - MainProcess - INFO     OpenAIEmbeddings object created successfully. (langchain_indexing.py:load_embedding_model:116)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='foundational-ada', openai_api_version='2023-05-15', openai_api_base='https://ml-workspace-dev-eastus-001-aoai.openai.azure.com/', openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='d050ad8b96ef4ecbb5099eece1212a91', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=16, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=True, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPLOYMENT =\"foundational-ada\"\n",
    "MODEL_NAME=\"text-embedding-ada-002\"\n",
    "client_indexing.load_embedding_model(deployment=DEPLOYMENT,model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.90it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You need to specify at least the following fields {'content_vector': 'Collection(Edm.Single)'} or provide alternative field names in the env variables.\n\ncontent_vector current type: 'Edm.String'. It has to be 'Collection(Edm.Single)' or you can point to a different 'Collection(Edm.Single)' field name by using the env variable 'AZURESEARCH_FIELDS_CONTENT_VECTOR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:126\u001b[0m, in \u001b[0;36m_get_search_client\u001b[1;34m(endpoint, key, index_name, semantic_configuration_name, fields, vector_search, semantic_settings, scoring_profiles, default_scoring_profile, default_fields, user_agent, cors_options)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     index_client\u001b[39m.\u001b[39;49mget_index(name\u001b[39m=\u001b[39;49mindex_name)\n\u001b[0;32m    127\u001b[0m \u001b[39mexcept\u001b[39;00m ResourceNotFoundError:\n\u001b[0;32m    128\u001b[0m     \u001b[39m# Fields configuration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\azure\\search\\documents\\indexes\\_search_index_client.py:136\u001b[0m, in \u001b[0;36mSearchIndexClient.get_index\u001b[1;34m(self, name, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_client_headers(kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> 136\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mindexes\u001b[39m.\u001b[39mget(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m SearchIndex\u001b[39m.\u001b[39m_from_generated(result)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\azure\\search\\documents\\indexes\\_generated\\operations\\_indexes_operations.py:877\u001b[0m, in \u001b[0;36mIndexesOperations.get\u001b[1;34m(self, index_name, request_options, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[1;32m--> 877\u001b[0m     map_error(status_code\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mstatus_code, response\u001b[39m=\u001b[39;49mresponse, error_map\u001b[39m=\u001b[39;49merror_map)\n\u001b[0;32m    878\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize\u001b[39m.\u001b[39mfailsafe_deserialize(_models\u001b[39m.\u001b[39mSearchError, pipeline_response)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\azure\\core\\exceptions.py:165\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    164\u001b[0m error \u001b[39m=\u001b[39m error_type(response\u001b[39m=\u001b[39mresponse)\n\u001b[1;32m--> 165\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[1;31mResourceNotFoundError\u001b[0m: () No index with the name 'langchain-vector-demo-custom' was found in the service 'azure-ai-search-dev-eastus-001'.\nCode: \nMessage: No index with the name 'langchain-vector-demo-custom' was found in the service 'azure-ai-search-dev-eastus-001'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search\\02-indexing-content-trimming.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pablosal/Desktop/sharepoint-indexing-azure-cognitive-search/02-indexing-content-trimming.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Tis fucntion creates index in Azure AI Search if not existeen and laod configuration - please modify the function if needed quickguide how below\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pablosal/Desktop/sharepoint-indexing-azure-cognitive-search/02-indexing-content-trimming.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m client_indexing\u001b[39m.\u001b[39;49msetup_azure_search(index_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlangchain-vector-demo-custom\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search\\gbb_ai\\langchain_indexing.py:178\u001b[0m, in \u001b[0;36mTextChunkingIndexing.setup_azure_search\u001b[1;34m(self, endpoint, admin_key, index_name)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOpenAIEmbeddings object has not been configured. Please call load_embedding_model() first.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    163\u001b[0m fields \u001b[39m=\u001b[39m [\n\u001b[0;32m    164\u001b[0m     SimpleField(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mSearchFieldDataType\u001b[39m.\u001b[39mString, key\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m    165\u001b[0m     SearchableField(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mSearchFieldDataType\u001b[39m.\u001b[39mString, searchable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     SimpleField(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msecurity_group\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mSearchFieldDataType\u001b[39m.\u001b[39mString, filterable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m    176\u001b[0m ]\n\u001b[1;32m--> 178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_store \u001b[39m=\u001b[39m AzureSearch(\n\u001b[0;32m    179\u001b[0m     azure_search_endpoint\u001b[39m=\u001b[39;49mresolved_endpoint,\n\u001b[0;32m    180\u001b[0m     azure_search_key\u001b[39m=\u001b[39;49mresolved_admin_key,\n\u001b[0;32m    181\u001b[0m     index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[0;32m    182\u001b[0m     embedding_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49membed_query,\n\u001b[0;32m    183\u001b[0m     fields\u001b[39m=\u001b[39;49mfields,\n\u001b[0;32m    184\u001b[0m     semantic_settings\u001b[39m=\u001b[39;49mSemanticSettings(\n\u001b[0;32m    185\u001b[0m         default_configuration\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    186\u001b[0m         configurations\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    187\u001b[0m             SemanticConfiguration(\n\u001b[0;32m    188\u001b[0m                 name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    189\u001b[0m                 prioritized_fields\u001b[39m=\u001b[39;49mPrioritizedFields(\n\u001b[0;32m    190\u001b[0m                     title_field\u001b[39m=\u001b[39;49mSemanticField(field_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    191\u001b[0m                     prioritized_content_fields\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    192\u001b[0m                         SemanticField(field_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    193\u001b[0m                     ],\n\u001b[0;32m    194\u001b[0m                     prioritized_keywords_fields\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    195\u001b[0m                         SemanticField(field_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    196\u001b[0m                     ],\n\u001b[0;32m    197\u001b[0m                 ),\n\u001b[0;32m    198\u001b[0m             )\n\u001b[0;32m    199\u001b[0m         ],\n\u001b[0;32m    200\u001b[0m     ),\n\u001b[0;32m    201\u001b[0m )\n\u001b[0;32m    203\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAzure Cognitive Search client configured successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_store\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:300\u001b[0m, in \u001b[0;36mAzureSearch.__init__\u001b[1;34m(self, azure_search_endpoint, azure_search_key, index_name, embedding_function, search_type, semantic_configuration_name, semantic_query_language, fields, vector_search, semantic_settings, scoring_profiles, default_scoring_profile, cors_options, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    299\u001b[0m     user_agent \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m _get_search_client(\n\u001b[0;32m    301\u001b[0m     azure_search_endpoint,\n\u001b[0;32m    302\u001b[0m     azure_search_key,\n\u001b[0;32m    303\u001b[0m     index_name,\n\u001b[0;32m    304\u001b[0m     semantic_configuration_name\u001b[39m=\u001b[39;49msemantic_configuration_name,\n\u001b[0;32m    305\u001b[0m     fields\u001b[39m=\u001b[39;49mfields,\n\u001b[0;32m    306\u001b[0m     vector_search\u001b[39m=\u001b[39;49mvector_search,\n\u001b[0;32m    307\u001b[0m     semantic_settings\u001b[39m=\u001b[39;49msemantic_settings,\n\u001b[0;32m    308\u001b[0m     scoring_profiles\u001b[39m=\u001b[39;49mscoring_profiles,\n\u001b[0;32m    309\u001b[0m     default_scoring_profile\u001b[39m=\u001b[39;49mdefault_scoring_profile,\n\u001b[0;32m    310\u001b[0m     default_fields\u001b[39m=\u001b[39;49mdefault_fields,\n\u001b[0;32m    311\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    312\u001b[0m     cors_options\u001b[39m=\u001b[39;49mcors_options,\n\u001b[0;32m    313\u001b[0m )\n\u001b[0;32m    314\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m=\u001b[39m search_type\n\u001b[0;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemantic_configuration_name \u001b[39m=\u001b[39m semantic_configuration_name\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:150\u001b[0m, in \u001b[0;36m_get_search_client\u001b[1;34m(endpoint, key, index_name, semantic_configuration_name, fields, vector_search, semantic_settings, scoring_profiles, default_scoring_profile, default_fields, user_agent, cors_options)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    143\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m current type: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfields_types\u001b[39m.\u001b[39mget(x,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mMISSING\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIt has to be \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmandatory_fields\u001b[39m.\u001b[39mget(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m or you can point \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto a different \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmandatory_fields\u001b[39m.\u001b[39mget(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m field name by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39musing the env variable \u001b[39m\u001b[39m'\u001b[39m\u001b[39mAZURESEARCH_FIELDS_\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m             )\n\u001b[0;32m    149\u001b[0m         error \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([fmt_err(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m missing_fields])\n\u001b[1;32m--> 150\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to specify at least the following fields \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmissing_fields\u001b[39m}\u001b[39;00m\u001b[39m or provide alternative field names in the env \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvariables.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         )\n\u001b[0;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     fields \u001b[39m=\u001b[39m default_fields\n",
      "\u001b[1;31mValueError\u001b[0m: You need to specify at least the following fields {'content_vector': 'Collection(Edm.Single)'} or provide alternative field names in the env variables.\n\ncontent_vector current type: 'Edm.String'. It has to be 'Collection(Edm.Single)' or you can point to a different 'Collection(Edm.Single)' field name by using the env variable 'AZURESEARCH_FIELDS_CONTENT_VECTOR'"
     ]
    }
   ],
   "source": [
    "# Tis fucntion creates index in Azure AI Search if not existeen and laod configuration - please modify the function if needed quickguide how below\n",
    "client_indexing.setup_azure_search(index_name=\"langchain-vector-demo-custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Guide to Setting Up Azure Search Index\n",
    "\n",
    "Let's set up an Azure Search index tailored for advanced search capabilities, including semantic and vector-based searches. Here's a step-by-step guide:\n",
    "\n",
    "##### Embedding Function Setup:\n",
    "\n",
    "Define embedding_function to transform text into vectors. This powers the semantic search.\n",
    "\n",
    "##### Define Index Fields:\n",
    "\n",
    "Create fields like id, content, content_vector, and others in the fields list. Each field represents a document attribute.\n",
    "Make sure content_vector aligns with your embedding function's output.\n",
    "\n",
    "##### Initialize Azure Search Client:\n",
    "\n",
    "Instantiate AzureSearch with your Azure endpoint, key, custom index_name, and the fields list.\n",
    "Configure semantic settings to fine-tune search relevance.\n",
    "\n",
    "##### Customize As Needed:\n",
    "\n",
    "Modify fields based on your document attributes.\n",
    "Adjust index_name or semantic configurations to fit your specific search needs.\n",
    "\n",
    "```python \n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchFieldDataType, SimpleField, SearchableField, SemanticSettings, SemanticConfiguration, PrioritizedFields, SemanticField\n",
    ")\n",
    "from azure.search.documents.models import Vector\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from your_embedding_module import embeddings  # Replace with your actual module\n",
    "\n",
    "# Embedding function and fields setup\n",
    "embedding_function = embeddings.embed_query\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    # ... other fields ...\n",
    "]\n",
    "\n",
    "# Azure Search client initialization\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"),\n",
    "    index_name=\"your-custom-index-name\",\n",
    "    embedding_function=embedding_function,\n",
    "    fields=fields,\n",
    "    # Semantic settings\n",
    "    semantic_settings=SemanticSettings(\n",
    "        default_configuration=\"config\",\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"config\",\n",
    "                prioritized_fields=PrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"content\"),\n",
    "                    # ... other configurations ...\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Now, your Azure Search index is ready for advanced querying!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chuncks = client_indexing.split_documents_by_character(content_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.71it/s]\n"
     ]
    }
   ],
   "source": [
    "client_indexing.embed_and_index(texts=chuncks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r'C:\\Users\\pablosal\\Desktop\\sharepoint-indexing-azure-cognitive-search'\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbb_ai.trimming_ai_search import AzureSearchManager\n",
    "\n",
    "client_search = AzureSearchManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"LLM is a master of laws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\sharepoint-indexing\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "2023-12-09 20:14:56,605 - micro - MainProcess - INFO     Search query: LLM is a master of laws, results: [{'score': 0.02812499925494194, 'reranker_score': 2.2067630290985107, 'content': 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5] Notable exam'}, {'score': 0.027313265949487686, 'reranker_score': 2.2067630290985107, 'content': 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5]'}, {'score': 0.02581612393260002, 'reranker_score': 2.2067630290985107, 'content': 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5] Notable exam'}, {'score': 0.021915584802627563, 'reranker_score': 2.2067630290985107, 'content': 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5]'}, {'score': 0.021193092688918114, 'reranker_score': 2.0301260948181152, 'content': 'Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[70] Let � be the number of parameter count, and � be the performance of the model. When �=average ��(correct token), then (log\\u2061�,�) is an exponential curve (before it hits the plateau at one), which looks like emergence. When �=average log\\u2061(��(correct token)), then the (log\\u2061�,�) plot is a straight line (before it hits the plateau at zero), which does not look like emergence. When �=average ��(the most likely token is correct), then (log\\u2061�,�) is a step-function, which looks like emergence. Interpretation[edit]'}] (trimming_ai_search.py:hybrid_retrieval_rerank:78)\n"
     ]
    }
   ],
   "source": [
    "results = client_search.hybrid_retrieval_rerank(search_query=search_query, security_group=\"Group_critical\", top_k=5, azure_deployment_name=\"foundational-ada\", semantic_configuration_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5] Notable exam',\n",
       " 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5]',\n",
       " 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5] Notable exam',\n",
       " 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning. As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5]',\n",
       " 'Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[70] Let � be the number of parameter count, and � be the performance of the model. When �=average ��(correct token), then (log\\u2061�,�) is an exponential curve (before it hits the plateau at one), which looks like emergence. When �=average log\\u2061(��(correct token)), then the (log\\u2061�,�) plot is a straight line (before it hits the plateau at zero), which does not look like emergence. When �=average ��(the most likely token is correct), then (log\\u2061�,�) is a step-function, which looks like emergence. Interpretation[edit]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharepoint-indexing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
